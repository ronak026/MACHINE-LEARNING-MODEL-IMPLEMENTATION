{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spam Email Detection using Machine Learning\n",
        "\n",
        "This notebook demonstrates the implementation of a predictive model using scikit-learn to classify emails as spam or ham (non-spam).\n",
        "\n",
        "## Task Requirements\n",
        "- ✅ Create a predictive model using scikit-learn\n",
        "- ✅ Classify/predict outcomes from a dataset (spam email detection)\n",
        "- ✅ Showcase model implementation\n",
        "- ✅ Showcase model evaluation\n",
        "\n",
        "## Objectives\n",
        "- Load and explore email dataset\n",
        "- Preprocess text data using TF-IDF vectorization\n",
        "- Train multiple classification models\n",
        "- Evaluate and compare model performance\n",
        "- Visualize results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, \n",
        "    precision_score, \n",
        "    recall_score, \n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Exploration\n",
        "\n",
        "We'll create a sample dataset for demonstration. In a real-world scenario, you would load data from a CSV file or database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample email dataset\n",
        "# In production, load from: df = pd.read_csv('spam_emails.csv')\n",
        "\n",
        "sample_emails = [\n",
        "    # Spam emails\n",
        "    (\"WINNER!! You have won $1,000,000! Click here to claim your prize now!\", \"spam\"),\n",
        "    (\"URGENT: Your account will be suspended. Verify your details immediately.\", \"spam\"),\n",
        "    (\"Free money! No investment required. Get rich quick scheme.\", \"spam\"),\n",
        "    (\"Congratulations! You've been selected for a free iPhone. Claim now!\", \"spam\"),\n",
        "    (\"Limited time offer! Buy now and get 90% discount. Act fast!\", \"spam\"),\n",
        "    (\"You have won a lottery! Claim your prize worth $500,000 today.\", \"spam\"),\n",
        "    (\"Click here for amazing deals! Lowest prices guaranteed.\", \"spam\"),\n",
        "    (\"Your payment failed. Update your credit card information now.\", \"spam\"),\n",
        "    (\"Exclusive offer just for you! Don't miss this opportunity.\", \"spam\"),\n",
        "    (\"Act now! Limited stock available. Order before it's too late.\", \"spam\"),\n",
        "    (\"You've been pre-approved for a loan. Apply now with no credit check.\", \"spam\"),\n",
        "    (\"Free trial! Cancel anytime. Sign up now for premium access.\", \"spam\"),\n",
        "    (\"Your package delivery failed. Click to reschedule delivery.\", \"spam\"),\n",
        "    (\"Earn money from home! Work from home opportunity. No experience needed.\", \"spam\"),\n",
        "    (\"Special promotion! Buy one get one free. Limited time only.\", \"spam\"),\n",
        "    \n",
        "    # Ham (non-spam) emails\n",
        "    (\"Hi, can we schedule a meeting for tomorrow afternoon?\", \"ham\"),\n",
        "    (\"Thank you for your email. I'll get back to you soon.\", \"ham\"),\n",
        "    (\"The project deadline has been extended to next Friday.\", \"ham\"),\n",
        "    (\"Please find attached the report you requested.\", \"ham\"),\n",
        "    (\"Let's discuss the quarterly results in our next team meeting.\", \"ham\"),\n",
        "    (\"I'll be out of office next week. Please contact my assistant.\", \"ham\"),\n",
        "    (\"The conference call is scheduled for 3 PM today.\", \"ham\"),\n",
        "    (\"Could you please review the document and provide feedback?\", \"ham\"),\n",
        "    (\"I've completed the analysis. Here are the key findings.\", \"ham\"),\n",
        "    (\"Thanks for your help with the project. Much appreciated!\", \"ham\"),\n",
        "    (\"The meeting has been rescheduled to next Monday at 10 AM.\", \"ham\"),\n",
        "    (\"Please confirm your attendance for the workshop next week.\", \"ham\"),\n",
        "    (\"I've updated the spreadsheet with the latest data.\", \"ham\"),\n",
        "    (\"Let me know if you need any additional information.\", \"ham\"),\n",
        "    (\"Great work on the presentation! The client was impressed.\", \"ham\"),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(sample_emails, columns=['email', 'label'])\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(df.head(10))\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(f\"\\nLabel distribution percentage:\")\n",
        "print(df['label'].value_counts(normalize=True) * 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize label distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Count plot\n",
        "sns.countplot(data=df, x='label', ax=axes[0])\n",
        "axes[0].set_title('Email Label Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Label', fontsize=12)\n",
        "axes[0].set_ylabel('Count', fontsize=12)\n",
        "\n",
        "# Pie chart\n",
        "df['label'].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=axes[1])\n",
        "axes[1].set_title('Label Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display sample emails\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Sample Spam Emails:\")\n",
        "print(\"=\"*80)\n",
        "for idx, row in df[df['label'] == 'spam'].head(3).iterrows():\n",
        "    print(f\"\\n{idx+1}. {row['email']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Sample Ham Emails:\")\n",
        "print(\"=\"*80)\n",
        "for idx, row in df[df['label'] == 'ham'].head(3).iterrows():\n",
        "    print(f\"\\n{idx+1}. {row['email']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "We need to:\n",
        "1. Convert text to numerical features using TF-IDF vectorization\n",
        "2. Encode labels (spam/ham) to numerical values\n",
        "3. Split data into training and testing sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features (emails) and labels\n",
        "X = df['email']\n",
        "y = df['label']\n",
        "\n",
        "# Encode labels: spam = 1, ham = 0\n",
        "y_encoded = (y == 'spam').astype(int)\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Labels shape: {y_encoded.shape}\")\n",
        "print(f\"\\nEncoded labels distribution:\")\n",
        "print(f\"Ham (0): {(y_encoded == 0).sum()}\")\n",
        "print(f\"Spam (1): {(y_encoded == 1).sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TF-IDF Vectorization\n",
        "# Converts text into numerical features based on term frequency-inverse document frequency\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=1000,  # Maximum number of features\n",
        "    stop_words='english',  # Remove common English stopwords\n",
        "    lowercase=True,  # Convert to lowercase\n",
        "    ngram_range=(1, 2)  # Use unigrams and bigrams\n",
        ")\n",
        "\n",
        "# Transform emails to feature vectors\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "print(f\"Original text shape: {X.shape}\")\n",
        "print(f\"Vectorized shape: {X_vectorized.shape}\")\n",
        "print(f\"Number of features: {X_vectorized.shape[1]}\")\n",
        "print(f\"\\nSample feature names (first 20):\")\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(feature_names[:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "# 80% for training, 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_vectorized, \n",
        "    y_encoded, \n",
        "    test_size=0.2, \n",
        "    random_state=42,\n",
        "    stratify=y_encoded  # Maintain class distribution\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTraining set label distribution:\")\n",
        "print(f\"  Ham (0): {(y_train == 0).sum()}\")\n",
        "print(f\"  Spam (1): {(y_train == 1).sum()}\")\n",
        "print(f\"\\nTest set label distribution:\")\n",
        "print(f\"  Ham (0): {(y_test == 0).sum()}\")\n",
        "print(f\"  Spam (1): {(y_test == 1).sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Implementation and Training\n",
        "\n",
        "We'll train multiple classification models and compare their performance:\n",
        "- **Naive Bayes**: Good for text classification, fast and efficient\n",
        "- **Logistic Regression**: Simple and interpretable\n",
        "- **Random Forest**: Ensemble method, robust to overfitting\n",
        "- **Support Vector Machine (SVM)**: Effective for high-dimensional data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM': SVC(kernel='linear', random_state=42, probability=True)\n",
        "}\n",
        "\n",
        "# Train all models\n",
        "trained_models = {}\n",
        "print(\"Training models...\\n\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    trained_models[name] = model\n",
        "    print(f\"✓ {name} trained successfully!\\n\")\n",
        "\n",
        "print(\"All models trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation\n",
        "\n",
        "We'll evaluate each model using multiple metrics:\n",
        "- **Accuracy**: Overall correctness\n",
        "- **Precision**: How many predicted spams were actually spam\n",
        "- **Recall**: How many actual spams were correctly identified\n",
        "- **F1-Score**: Harmonic mean of precision and recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate all models\n",
        "results = []\n",
        "\n",
        "for name, model in trained_models.items():\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    \n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1\n",
        "    })\n",
        "    \n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1-Score:  {f1:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"=\"*80)\n",
        "print(\"Summary of All Models:\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    bars = ax.bar(results_df['Model'], results_df[metric], color=colors[idx], alpha=0.8)\n",
        "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel(metric, fontsize=12)\n",
        "    ax.set_ylim([0, 1.1])\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.3f}',\n",
        "                ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrices for all models\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, (name, model) in enumerate(trained_models.items()):\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
        "    axes[idx].set_title(f'{name} - Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Predicted', fontsize=11)\n",
        "    axes[idx].set_ylabel('Actual', fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Detailed Classification Report\n",
        "\n",
        "Let's examine the best performing model in detail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best model based on F1-Score\n",
        "best_model_name = results_df.loc[results_df['F1-Score'].idxmax(), 'Model']\n",
        "best_model = trained_models[best_model_name]\n",
        "\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Detailed classification report\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_best, target_names=['Ham', 'Spam']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Testing with New Emails\n",
        "\n",
        "Let's test the best model with some new email examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# New test emails\n",
        "new_emails = [\n",
        "    \"Congratulations! You've won a free vacation. Click here to claim!\",\n",
        "    \"Hi John, can we meet tomorrow to discuss the project?\",\n",
        "    \"URGENT: Your account has been compromised. Verify immediately!\",\n",
        "    \"Thanks for the update. I'll review the document and get back to you.\",\n",
        "    \"Get rich quick! Earn $5000 per week from home. No experience needed!\",\n",
        "    \"The meeting is scheduled for 2 PM in the conference room.\",\n",
        "]\n",
        "\n",
        "print(f\"Testing {best_model_name} with new emails:\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for email in new_emails:\n",
        "    # Vectorize the email\n",
        "    email_vectorized = vectorizer.transform([email])\n",
        "    \n",
        "    # Make prediction\n",
        "    prediction = best_model.predict(email_vectorized)[0]\n",
        "    prediction_proba = best_model.predict_proba(email_vectorized)[0]\n",
        "    \n",
        "    label = \"SPAM\" if prediction == 1 else \"HAM\"\n",
        "    confidence = prediction_proba[1] if prediction == 1 else prediction_proba[0]\n",
        "    \n",
        "    print(f\"\\nEmail: {email}\")\n",
        "    print(f\"Prediction: {label}\")\n",
        "    print(f\"Confidence: {confidence:.2%}\")\n",
        "    print(f\"Probabilities - Ham: {prediction_proba[0]:.2%}, Spam: {prediction_proba[1]:.2%}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Importance Analysis\n",
        "\n",
        "Let's examine which words/features are most important for spam detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance (for models that support it)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    # Random Forest\n",
        "    feature_importance = best_model.feature_importances_\n",
        "elif hasattr(best_model, 'coef_'):\n",
        "    # Logistic Regression or SVM\n",
        "    feature_importance = np.abs(best_model.coef_[0])\n",
        "else:\n",
        "    # Naive Bayes - use log probabilities\n",
        "    feature_importance = np.abs(best_model.feature_log_prob_[1] - best_model.feature_log_prob_[0])\n",
        "\n",
        "# Get top features\n",
        "top_n = 20\n",
        "top_indices = np.argsort(feature_importance)[-top_n:][::-1]\n",
        "top_features = [(feature_names[i], feature_importance[i]) for i in top_indices]\n",
        "\n",
        "print(f\"Top {top_n} Most Important Features for Spam Detection:\")\n",
        "print(\"=\"*80)\n",
        "for feature, importance in top_features:\n",
        "    print(f\"{feature:30s} : {importance:.4f}\")\n",
        "\n",
        "# Visualize top features\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "features, importances = zip(*top_features)\n",
        "y_pos = np.arange(len(features))\n",
        "\n",
        "ax.barh(y_pos, importances, color='steelblue', alpha=0.8)\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(features)\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel('Importance', fontsize=12)\n",
        "ax.set_title(f'Top {top_n} Features for Spam Detection ({best_model_name})', \n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Conclusions\n",
        "\n",
        "### Key Findings:\n",
        "1. **Model Performance**: All models achieved high accuracy on the test set\n",
        "2. **Best Model**: Determined based on F1-Score\n",
        "3. **Key Features**: Words like \"free\", \"win\", \"urgent\", \"click\" are strong indicators of spam\n",
        "\n",
        "### Model Comparison:\n",
        "- **Naive Bayes**: Fast and efficient, good baseline\n",
        "- **Logistic Regression**: Simple and interpretable\n",
        "- **Random Forest**: Robust, handles non-linear relationships\n",
        "- **SVM**: Effective for high-dimensional sparse data\n",
        "\n",
        "### Recommendations:\n",
        "- For production, consider using ensemble methods\n",
        "- Regularly retrain the model with new data\n",
        "- Monitor false positives and false negatives\n",
        "- Consider using deep learning for larger datasets\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
